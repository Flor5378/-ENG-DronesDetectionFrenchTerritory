{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f58011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGES PYTHON\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium import plugins\n",
    "import pandas as pd\n",
    "import json\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from geopy.distance import geodesic\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996486e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV chargé avec succès.\n",
      "Dimensions du DataFrame : (625500, 7)\n",
      "CSV chargé avec succès.\n",
      "Dimensions du DataFrame : (625500, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "df = None  # Déclaration de la variable globale\n",
    "\n",
    "def charger_csv():\n",
    "    global df  # Indique qu'on utilise la variable globale\n",
    "    filepath = filedialog.askopenfilename(\n",
    "        title=\"Choisir un fichier CSV\",\n",
    "        filetypes=[(\"Fichiers CSV\", \"*.csv\")],\n",
    "    )\n",
    "    if filepath:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep=';')\n",
    "            print(\"CSV chargé avec succès.\")\n",
    "            print(\"Dimensions du DataFrame :\", df.shape)\n",
    "        except Exception as e:\n",
    "            print(\"Erreur lors du chargement du fichier :\", e)\n",
    "\n",
    "# Interface de base avec Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Lecteur CSV\")\n",
    "root.geometry(\"300x100\")\n",
    "\n",
    "bouton_charger = tk.Button(root, text=\"Charger un CSV\", command=charger_csv)\n",
    "bouton_charger.pack(pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724d1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_json_str(s):\n",
    "    if s.startswith('{drone\":'):\n",
    "        s = '{\"' + s[1:]\n",
    "    if s.endswith('}}\"'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "def extract_values(s):\n",
    "    try:\n",
    "        s_clean = clean_json_str(s)\n",
    "        d = json.loads(s_clean)\n",
    "        drone = d.get('drone', {})\n",
    "        altitude = drone.get('altitude', None)\n",
    "        hauteur = drone.get('hauteur', None)\n",
    "        vitesse = drone.get('vitesse', None)\n",
    "        return altitude, hauteur, vitesse\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur extraction valeurs : {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Appliquer sur toute la colonne properties\n",
    "df[['altitude', 'hauteur', 'vitesse']] = df['properties'].apply(\n",
    "    lambda s: pd.Series(extract_values(s))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e3669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la colonne 'properties' du DataFrame original\n",
    "df_initial_sans_properties = df\n",
    "\n",
    "# Faire la concaténation côte à côte\n",
    "df_drones = pd.concat([df_initial_sans_properties], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de572545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "colonnes_voulues = [\n",
    "    'drone_id',\n",
    "    'constructeur',\n",
    "    'modele',\n",
    "    'updated',  # temps\n",
    "    'y',\n",
    "    'x',\n",
    "    'altitude',\n",
    "    'hauteur',\n",
    "    'vitesse',\n",
    "    'properties'\n",
    "]\n",
    "\n",
    "df_kepler_ready = df_drones[colonnes_voulues].copy()\n",
    "\n",
    "df_kepler_ready.to_csv('drones_kepler.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fad6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "df['updated'] = pd.to_datetime(df['updated'], utc=True, errors='coerce')\n",
    "print(df['updated'].dtypes)  # Doit afficher le type de date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a05509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assure-toi que 'updated' est datetime (avec fuseau horaire ou en UTC)\n",
    "df['updated'] = pd.to_datetime(df['updated'], utc=True, errors='coerce')\n",
    "\n",
    "# Nettoyer la colonne 'vitesse'\n",
    "df['vitesse'] = pd.to_numeric(df['vitesse'], errors='coerce').fillna(0)\n",
    "# Trier les données pour plus de clarté\n",
    "df = df.sort_values(['drone_id', 'updated'])\n",
    "\n",
    "# Liste des drones uniques\n",
    "drone_ids = df['drone_id'].unique()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e0430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le fichier CSV\n",
    "df_clean = pd.read_csv('drones_kepler.csv')\n",
    "# Vérification rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c78ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fgrol\\AppData\\Local\\Temp\\ipykernel_18864\\3456638988.py:2: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_clean['updated'] = pd.to_datetime(df_clean['updated'])\n"
     ]
    }
   ],
   "source": [
    "# S'assurer que 'updated' est bien en datetime\n",
    "df_clean['updated'] = pd.to_datetime(df_clean['updated'])\n",
    "\n",
    "# Trier les données par drone et timestamp\n",
    "df_clean = df_clean.sort_values(by=['drone_id', 'updated']).reset_index(drop=True)\n",
    "\n",
    "# Initialiser la colonne des vitesses calculées\n",
    "df_clean['vitesse_calculee'] = 0.0\n",
    "\n",
    "# Grouper par drone_id pour calculer la vitesse entre deux points consécutifs\n",
    "for drone_id, group in df_clean.groupby('drone_id'):\n",
    "    idxs = group.index\n",
    "    for i in range(1, len(group)):\n",
    "        idx_prev = idxs[i - 1]\n",
    "        idx_curr = idxs[i]\n",
    "\n",
    "        # Extraire les coordonnées (lat, lon)\n",
    "        coord_prev = (df_clean.loc[idx_prev, 'y'], df_clean.loc[idx_prev, 'x'])\n",
    "        coord_curr = (df_clean.loc[idx_curr, 'y'], df_clean.loc[idx_curr, 'x'])\n",
    "\n",
    "        # Calcul de la distance (mètres)\n",
    "        distance = geodesic(coord_prev, coord_curr).meters\n",
    "\n",
    "        # Calcul du temps (secondes)\n",
    "        t1 = df_clean.loc[idx_prev, 'updated']\n",
    "        t2 = df_clean.loc[idx_curr, 'updated']\n",
    "        delta_t = (t2 - t1).total_seconds()\n",
    "\n",
    "        # Calcul de la vitesse (m/s)\n",
    "        vitesse = distance / delta_t if delta_t > 0 else 0.0\n",
    "\n",
    "        df_clean.loc[idx_curr, 'vitesse_calculee'] = vitesse\n",
    "\n",
    "# Nettoyer la colonne 'vitesse' (s'assurer qu'elle est numérique)\n",
    "df_clean['vitesse'] = pd.to_numeric(df_clean['vitesse'], errors='coerce')\n",
    "\n",
    "# Remplacer les valeurs manquantes (NaN) dans 'vitesse' par 'vitesse_calculee'\n",
    "df_clean['vitesse'] = df_clean['vitesse'].fillna(df_clean['vitesse_calculee'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee0aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# 1. Convertir 'updated' en datetime avec UTC\n",
    "df_clean['updated'] = pd.to_datetime(df_clean['updated'], utc=True)\n",
    "\n",
    "# 2. Trier par drone_id et timestamp\n",
    "df_clean = df_clean.sort_values(by=['drone_id', 'updated']).reset_index(drop=True)\n",
    "\n",
    "# 3. Calcul du temps écoulé (delta_t) en secondes\n",
    "df_clean['delta_t'] = df_clean.groupby('drone_id')['updated'].diff().dt.total_seconds()\n",
    "\n",
    "# 4. Calcul de la vitesse verticale vz (altitude / temps)\n",
    "df_clean['vz'] = df_clean.groupby('drone_id')['hauteur'].diff() / df_clean['delta_t']\n",
    "\n",
    "# 5. Calcul de la distance entre points GPS successifs (avec x = lon, y = lat)\n",
    "def compute_distance(row):\n",
    "    i = row.name\n",
    "    if i == 0 or df_clean.loc[i, 'drone_id'] != df_clean.loc[i - 1, 'drone_id']:\n",
    "        return 0\n",
    "    coord1 = (df_clean.loc[i - 1, 'y'], df_clean.loc[i - 1, 'x'])  # (lat, lon)\n",
    "    coord2 = (df_clean.loc[i, 'y'], df_clean.loc[i, 'x'])          # (lat, lon)\n",
    "    return geodesic(coord1, coord2).m\n",
    "\n",
    "df_clean['distance'] = df_clean.apply(compute_distance, axis=1)\n",
    "\n",
    "# 6. Détection de nouvelle trajectoire si delta_t > 5 minutes ou changement de drone\n",
    "df_clean['new_traj'] = (\n",
    "    (df_clean['delta_t'] > 300) |\n",
    "    (df_clean['drone_id'] != df_clean['drone_id'].shift())\n",
    ").astype(int)\n",
    "\n",
    "# 7. Attribution d’un ID de trajectoire unique\n",
    "df_clean['traj_id'] = df_clean.groupby('drone_id')['new_traj'].cumsum()\n",
    "\n",
    "# 8. Calcul du temps total et distance totale pour chaque trajectoire\n",
    "traj_stats = df_clean.groupby(['drone_id', 'traj_id']).agg(\n",
    "    temps_total=('delta_t', 'sum'),\n",
    "    distance_totale=('distance', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Nettoyer les colonnes potentielles en doublon avant merge\n",
    "for col in ['temps_total', 'distance_totale', 'temps_total_x', 'temps_total_y', 'distance_totale_x', 'distance_totale_y']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 9. Fusion des statistiques dans le dataframe principal\n",
    "df_clean = df_clean.merge(traj_stats, on=['drone_id', 'traj_id'], how='left')\n",
    "\n",
    "# 10. Nettoyage de la colonne temporaire\n",
    "df_clean.drop(columns=['new_traj'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252390ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assure que 'vz' est numérique\n",
    "df_clean['vz'] = pd.to_numeric(df_clean['vz'], errors='coerce').fillna(0)\n",
    "\n",
    "# Tri par drone, trajectoire et temps\n",
    "df_clean = df_clean.sort_values(['drone_id', 'traj_id', 'updated'])\n",
    "\n",
    "# Récupère tous les groupes uniques\n",
    "group_keys = list(df_clean.groupby(['drone_id', 'traj_id']).groups.keys())\n",
    "\n",
    "# Vérifie qu'il y a au moins 9 groupes\n",
    "if len(group_keys) >= 9:\n",
    "    # On récupère la 9ᵉ (index 8 car indexation commence à 0)\n",
    "    selected_key = group_keys[8]\n",
    "    drone, traj = selected_key\n",
    "\n",
    "    df_sub = df_clean[(df_clean['drone_id'] == drone) & (df_clean['traj_id'] == traj)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db090e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assurer que 'updated' est bien au format datetime\n",
    "df_clean['updated'] = pd.to_datetime(df_clean['updated'])\n",
    "\n",
    "# Obtenir les 10 premières combinaisons uniques\n",
    "top_10_traj = df_clean[['drone_id', 'traj_id']].drop_duplicates().head(10)\n",
    "\n",
    "# Vérifier qu'on a au moins 9 trajectoires\n",
    "if len(top_10_traj) >= 9:\n",
    "    # Récupérer la 9ᵉ trajectoire (index 8)\n",
    "    row = top_10_traj.iloc[8]\n",
    "    drone_id = row['drone_id']\n",
    "    traj_id = row['traj_id']\n",
    "\n",
    "    # Filtrer les données de cette trajectoire\n",
    "    group = df_clean[(df_clean['drone_id'] == drone_id) & (df_clean['traj_id'] == traj_id)]\n",
    "    group = group.sort_values('updated')\n",
    "\n",
    "    # Calcul de la distance totale\n",
    "    distance_totale = group['distance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346e2a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fgrol\\AppData\\Local\\Temp\\ipykernel_18864\\3502637546.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_clean = df_clean.groupby(['drone_id', 'traj_id'], group_keys=False).apply(compute_max_distance_to_start)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fonction pour calculer la distance max au point (x0, y0)\n",
    "def compute_max_distance_to_start(group):\n",
    "    x0, y0 = group.iloc[0][['x', 'y']]  # point de la télécommande\n",
    "    distances = np.sqrt((group['x'] - x0)**2 + (group['y'] - y0)**2)\n",
    "    max_distance = distances.max()\n",
    "    group['distance_max_telecommande'] = max_distance\n",
    "    return group\n",
    "\n",
    "# Appliquer la fonction par trajectoire\n",
    "df_clean = df_clean.groupby(['drone_id', 'traj_id'], group_keys=False).apply(compute_max_distance_to_start)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f215f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_json_str(s):\n",
    "    if s.startswith('{drone\":'):\n",
    "        s = '{\"' + s[1:]\n",
    "    if s.endswith('}}\"'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "def extract_lat_lon(s):\n",
    "    try:\n",
    "        s_clean = clean_json_str(s)\n",
    "        d = json.loads(s_clean)\n",
    "        drone = d.get('drone', {})\n",
    "        latitude_depart = drone.get('latitude_depart', None)\n",
    "        longitude_depart = drone.get('longitude_depart', None)\n",
    "        return latitude_depart, longitude_depart\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur extraction coordonnées : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Application sur df_clean\n",
    "df_clean[['latitude_depart', 'longitude_depart']] = df_clean['properties'].apply(\n",
    "    lambda s: pd.Series(extract_lat_lon(s))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb2db277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Définir les bornes valides de la France métropolitaine\n",
    "lat_min, lat_max = 41.0, 51.5\n",
    "lon_min, lon_max = -5.5, 9.5\n",
    "\n",
    "# Masques de validité\n",
    "lat_valid = df_clean['latitude_depart'].between(lat_min, lat_max)\n",
    "lon_valid = df_clean['longitude_depart'].between(lon_min, lon_max)\n",
    "\n",
    "# Remplacer par NaN si hors des bornes\n",
    "df_clean.loc[~lat_valid, 'latitude_depart'] = np.nan\n",
    "df_clean.loc[~lon_valid, 'longitude_depart'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d66bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # rayon terrestre en mètres\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c  # distance en mètres\n",
    "\n",
    "# Distance par rapport au point de départ GPS\n",
    "df_clean['distance_from_depart'] = df_clean.apply(\n",
    "    lambda row: haversine(row['latitude_depart'], row['longitude_depart'], row['y'], row['x']), axis=1\n",
    ")\n",
    "\n",
    "# Distance maximale par groupe\n",
    "df_clean['distance_depart'] = df_clean.groupby(['drone_id', 'traj_id'])['distance_from_depart'].transform('max')\n",
    "# Remplacer la colonne par une version formatée en string\n",
    "df_clean['distance_depart'] = df_clean['distance_depart'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "df_clean.drop(columns=['distance_from_depart'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b510c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "from rapidfuzz import process\n",
    "\n",
    "# === 1. Liste officielle des modèles ===\n",
    "modeles_ref = [\n",
    "    \"Mini 4K\", \"Mini 3 Pro\", \"Mini 3\", \"Mini 2 SE\", \"Mini SE\", \"Air 3S\", \"Air 3\", \"Air 2S\",\n",
    "    \"Mavic 4 Pro\", \"Mavic 3 Pro\", \"Mavic 3 Classic\", \"Mavic 3\", \"Mavic 2\", \"Mavic Pro\",\n",
    "    \"Mavic Pro Platinum\", \"Avata\", \"FPV (original)\", \"Inspire 3\", \"Inspire 2\",\n",
    "    \"Matrice 300 RTK\", \"Matrice 350 RTK\", \"Matrice 30\", \"Matrice 30T\", \"Matrice 400\",\n",
    "    \"Matrice 4E\", \"Matrice 4T\", \"Mavic 3T\", \"Mavic 3E\", \"Mavic 3M\",\n",
    "    \"Mavic 2 Enterprise Dual\", \"Mavic 2 Enterprise Advanced\", \"Agras T50\", \"Agras T25\",\n",
    "    \"T20\", \"T25P\", \"T30\", \"T60\", \"T100\", \"MG-1\", \"Phantom 4 Pro\", \"Phantom 4 Advanced\",\n",
    "    \"Spreading Wings S1000\", \"Evo Lite+\", \"Evo Nano+\", \"Typhoon H\", \"Typhoon H Plus\",\n",
    "    \"Breeze\", \"Ghost Drone 2.0\", \"Atom 2\", \"Anafi\", \"Anafi USA\", \"Mavic 2 Zoom\",\n",
    "    \"Phantom 4 RTK\", \"Extra 330S\"\n",
    "]\n",
    "\n",
    "# === 2. Fonction de nettoyage des chaînes ===\n",
    "def normalize(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unidecode.unidecode(s).lower()\n",
    "    s = s.replace(\"-\", \" \").replace(\"_\", \" \").strip()\n",
    "    return s\n",
    "\n",
    "# === 3. Normaliser la liste de référence ===\n",
    "normalized_ref = {normalize(m): m for m in modeles_ref}\n",
    "normalized_ref_list = list(normalized_ref.keys())\n",
    "\n",
    "# === 4. Fonction de correspondance floue robuste ===\n",
    "def match_model(s, threshold=85):\n",
    "    s_norm = normalize(s)\n",
    "    result = process.extractOne(s_norm, normalized_ref_list, score_cutoff=threshold)\n",
    "    if result:\n",
    "        match, score, _ = result\n",
    "        return normalized_ref[match]\n",
    "    return None\n",
    "\n",
    "# === 5. Optimisation : matching sur valeurs uniques ===\n",
    "valeurs_uniques = df_clean[\"modele\"].dropna().unique()\n",
    "correspondance = {val: match_model(val) for val in valeurs_uniques}\n",
    "\n",
    "# === 6. Remplacement dans le DataFrame ===\n",
    "df_clean[\"modele_standardise\"] = df_clean[\"modele\"].map(correspondance)\n",
    "\n",
    "# === 7. (Optionnel) Export des modèles non reconnus ===\n",
    "non_reconnus = {k: v for k, v in correspondance.items() if v is None}\n",
    "pd.Series(non_reconnus).to_csv(\"modeles_non_reconnus.csv\")\n",
    "\n",
    "df_clean.to_csv(\"csv_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66a3bb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Air 2S\n",
      "2: Air 3\n",
      "3: Air 3S\n",
      "4: Avata\n",
      "5: Extra 330S\n",
      "6: Inspire 3\n",
      "7: Matrice 300 RTK\n",
      "8: Matrice 30T\n",
      "9: Matrice 350 RTK\n",
      "10: Matrice 4E\n",
      "11: Matrice 4T\n",
      "12: Mavic 2\n",
      "13: Mavic 2 Enterprise Advanced\n",
      "14: Mavic 2 Enterprise Dual\n",
      "15: Mavic 2 Zoom\n",
      "16: Mavic 3\n",
      "17: Mavic 3 Classic\n",
      "18: Mavic 3 Pro\n",
      "19: Mavic 3E\n",
      "20: Mavic 3M\n",
      "21: Mavic 3T\n",
      "22: Mavic 4 Pro\n",
      "23: Mavic Pro\n",
      "24: Mini 2 SE\n",
      "25: Mini 3 Pro\n",
      "26: Mini 4K\n",
      "27: Mini SE\n",
      "28: Phantom 4 Pro\n",
      "29: Phantom 4 RTK\n"
     ]
    }
   ],
   "source": [
    "modeles_uniques = df_clean[\"modele_standardise\"].dropna().unique()\n",
    "modeles_uniques.sort()\n",
    "for i, modele in enumerate(modeles_uniques):\n",
    "    print(f\"{i+1}: {modele}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
